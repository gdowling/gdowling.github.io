---
redirect_from:
  - "09/02/01/ml9-2-03"
interact_link: content/09/02/01/ML9-2-03.ipynb
kernel_name: python3
kernel_path: content/09/02/01
has_widgets: false
title: |-
  Polynomial Regression
pagenum: 30
prev_page:
  url: /09/02/01/ML9-2-02.html
next_page:
  url: 
suffix: .ipynb
search: polynomial regression linear data k regressions fit model overfitting line order forms expression training same not below dataset value error much where b degree too high test results build off principles allow us dont does follow trends image displays lines fitted green red observe very difficult graph low hand able achieve smaller math equation y predicted outcome coefficients intercept simply predictors raised power second quadratic parabolic curve third cubic fourth quartic etc something aware performing closely set such perform well unseen select run risk best check compare accuracy between better likelihood

comment: "***PROGRAMMATICALLY GENERATED, DO NOT EDIT. SEE ORIGINAL FILES IN /content***"
---

    <main class="jupyter-page">
    <div id="page-info"><div id="page-title">Polynomial Regression</div>
</div>
    <div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Polynomial-Regression">Polynomial Regression<a class="anchor-link" href="#Polynomial-Regression"> </a></h2>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Polynomial regressions build off of many of the same principles from linear regressions. What polynomial regressions allow us to do that linear regression donâ€™t is fit to data that does not follow linear trends. The image below displays two regression lines fitted for the same dataset. A green line for the linear regression and red line for a polynomial regression. You can observe that it is very difficult to fit a linear regression line in the below graph with a low value of error. On the other hand, polynomial regression is able to achieve a much smaller error.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="k">import</span> <span class="n">Image</span>
<span class="n">Image</span><span class="p">(</span><span class="s2">&quot;PolyRegression.png&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea output_execute_result">
<img src="../../../images/09/02/01/ML9-2-03_2_0.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Math">Math<a class="anchor-link" href="#Math"> </a></h3><p>The equation for the polynomial regression is</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">Image</span><span class="p">(</span><span class="s2">&quot;PolyRegressionFormula.png&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea output_execute_result">
<img src="../../../images/09/02/01/ML9-2-03_4_0.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>where Y is the predicted outcome value for the polynomial model with regression coefficients b1 to k for each degree and intercept as b0. 
The model is simply a linear regression model with k predictors raised to the power of i where i=1 to k. A second order (k=2) polynomial forms a quadratic expression (parabolic curve), a third order (k=3) polynomial forms a cubic expression, and a fourth order (k=4) polynomial forms a quartic expression, etc.
Something to be aware of when performing polynomial regressions is overfitting. Overfitting is when you fit your data too closely to your training data set such that it will not perform well on unseen data. If you select too high if a polynomial degree you can run the risk of overfitting your dataset. The best way to check for overfitting is to compare your accuracy between your training data and your test data. If you training results are much better than your test results there is a high likelihood that you have over fit your model.</p>

</div>
</div>
</div>
</div>

 


    </main>
    